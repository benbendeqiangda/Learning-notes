bpmn：业务流程建模符号(BPMN - Business Process Modeling Notation)

BPMN定义了一个[业务流程图](https://baike.baidu.com/item/业务流程图)（Business Process Diagram）

activiti是一个业务流程管理引擎，会沿着设计者设计好的流程，一步一步的执行下去，直到终点。

###### sql

```
IF( expr1 , expr2 , expr3 )
```

expr1 的值为 TRUE，则返回值为 expr2 ，expr1 的值为FALSE，则返回值为 expr3

例：

```
SELECT IF(STRCMP("111","222"),"不相等","相等");
-> 不相等
```

查找出售价为 50 的书，如果是 java 书的话，就要标注为 已售完：

```
select *,if(book_name='java','已卖完','有货') as product_status from book where price =50
```

#### bpmn

- Sequence：（After an element is visited during process execution, **all outgoing sequence flow will be followed**. ）

- exclusive gateway：在使用独占网关时，只选择一个序列流。如果多序列流具有计算结果为 true 的条件，则为 XML 中定义的第一个条件(并且只有那个条件!)被选中来继续这个过程。如果无法选择序列流，则将引发异常。

- usertask：如果没有给出给定文本字符串是用户还是组的具体信息，则引擎默认为 group。

- Receive Task：当流程执行到达接收任务时，流程状态将被持久性存储。这意味着流程将保持这种等待状态，直到引擎接收到特定的消息，这将触发接收任务之后流程的继续。

  要继续当前正在接收任务上等待的流程实例，必须使用到达接收任务的执行的 id 调用 runtimeService.signal (executionId)。下面的代码片段展示了这在实践中是如何工作的:

  ```java
  ProcessInstance pi = runtimeService.startProcessInstanceByKey("receiveTask");
  Execution execution = runtimeService.createExecutionQuery()
    .processInstanceId(pi.getId())
    .activityId("waitState")
    .singleResult();
  assertNotNull(execution);
  
  runtimeService.signal(execution.getId());
  ```

  ```
  List<Task> tasks = taskService.createTaskQuery().taskAssignee("kermit").list();
  List<Task> tasks = taskService.createTaskQuery().taskCandidateUser("kermit");
  ```


activiti的7大马车

- RepositoryService：提供一系列管理流程部署和流程定义的API。
- RuntimeService：在流程运行时对流程实例进行管理与控制。
- TaskService：对流程任务进行管理，例如任务提醒、任务完成和创建任务等。
- IdentityService：提供对流程角色数据进行管理的API，这些角色数据包括用户组、用户及它们之间的关系。
- ManagementService：提供对流程引擎进行管理和维护的服务。
- HistoryService：对流程的历史数据进行操作，包括查询、删除这些历史数据。
- FormService：表单服务。
- ProcessEngine（流程引擎对象）

| 表分类       | 表名称                | 表含义                        |
| ------------ | --------------------- | ----------------------------- |
|              | act_evt_log           | 事件处理日志表                |
| 一般数据     | act_ge_bytearray      | 通用的流程定义和流程资源      |
|              | act_ge_property       | 系统相关属性                  |
| 流程历史记录 | act_hi_actinst        | 历史的流程实例                |
|              | act_hi_attachment     | 历史的流程附件                |
|              | act_hi_comment        | 历史的说明性信息              |
|              | act_hi_detail         | 历史的流程运行中的细节信息    |
|              | act_hi_identitylink   | 历史的流程运行过程中用户关系  |
|              | act_hi_procinst       | 历史的流程实例                |
|              | act_hi_taskinst       | 历史的任务实例                |
|              | act_hi_varinst        | 历史的流程运行中的变量信息    |
| 用户用户组表 | act_id_group          | 身份信息-组信息               |
|              | act_id_info           | 身份信息-组信息               |
|              | act_id_membership     | 身份信息-用户和组关系的中间表 |
|              | act_id_user           | 身份信息-用户信息             |
|              | act_procdef_info      | 死信任务                      |
| 流程定义表   | act_re_deployment     | 部署单元信息                  |
|              | act_re_model          | 模型信息                      |
|              | act_re_procdef        | 已部署的流程定义              |
| 运行实例表   | act_ru_deadletter_job | 执行失败任务表                |
|              | act_ru_event_subscr   | 运行时事件                    |
|              | act_ru_execution      | 运行时流程执行实例            |
|              | act_ru_identitylink   | 运行时用户关系信息            |
|              | act_ru_job            | 运行时作业                    |
|              | act_ru_suspended_job  | 运行时暂停任务                |
|              | act_ru_task           | 运行时任务                    |
|              | act_ru_timer_job      | 运行时定时任务                |
|              | act_ru_variable       | 运行时变量表                  |

具体的表结构及详细介绍可参考[Activiti数据库表结构](https://links.jianshu.com/go?to=https%3A%2F%2Fblog.csdn.net%2Fhj7jay%2Farticle%2Fdetails%2F51302829) 里面有详细的每个表介绍。

```
Activiti的后台是有数据库的支持，所有的表都以ACT_开头。 第二部分是表示表的用途的两个字母标识。 用途也和服务的API对应。
ACT_RE_*: 'RE'表示repository。 这个前缀的表包含了流程定义和流程静态资源 （图片，规则，等等）。
ACT_RU_*: 'RU'表示runtime。 这些运行时的表，包含流程实例，任务，变量，异步任务，等运行中的数据。 Activiti只在流程实例执行过程中保存这些数据， 在流程结束时就会删除这些记录。 这样运行时表可以一直很小速度很快。
ACT_ID_*: 'ID'表示identity。 这些表包含身份信息，比如用户，组等等。
ACT_HI_*: 'HI'表示history。 这些表包含历史数据，比如历史流程实例， 变量，任务等等。
ACT_GE_*: 通用数据， 用于不同场景下，如存放资源文件。
```

#### 新建工作流

画出bpmn流程图

- 修改name，key，id，name

- Sequence：（After an element is visited during process execution, **all outgoing sequence flow will be followed**. ）

- exclusive gateway：在使用独占网关时，只选择一个序列流。如果多序列流具有计算结果为 true 的条件，则为 XML 中定义的第一个条件(并且只有那个条件!)被选中来继续这个过程。如果无法选择序列流，则将引发异常。

- usertask：If no specifics are given whether the given text string is a user or group, the engine defaults to group. So the following would be the same as when *group(accountancy) was declared*.

  ```
  <formalExpression>accountancy</formalExpression>
  ```

  如果没有给出给定文本字符串是用户还是组的具体信息，则引擎默认为 group（）。

  

- Receive Task：当流程执行到达接收任务时，流程状态将被持久性存储。这意味着流程将保持这种等待状态，直到引擎接收到特定的消息，这将触发接收任务之后流程的继续。

- 去数据库添加工作流

- 添加模板和分类，绑定工作流

- 新建本地分支时记得指定原来的分支
  - git reflog 可以查看所有分支的所有操作记录（包括已经被删除的 commit 记录和 reset 的操作），拥有相同的编号
  
    例如执行 git reset --hard HEAD~1，退回到上一个版本，用git log则是看不出来被删除的commitid，用git reflog则可以看到被删除的commitid，我们就可以买后悔药，恢复到被删除的那个版本。
  
- 去项目管理系统添加代码模块
  - copyfrom：从哪个分支得基础上进行开发，一般是master
  - branch：代码模块拉取代码的分支
  
- 一键部署：容器部署，选择部署是上一次代码继续部署，代码改变后要选择编译部署

- 测试环境：虚机部署，

#### workorder新增节点

- 流入下一节点controller（解释和转交）：http://qa.docker.workorder.sogou/task/handleTask

  转交参数 `{"orderRecord":{"orderId":336773,"content":"转交参数测试","operate":"转交","operateType":2,"attachmentList":[]},"candidateGroups":[88],"candidateUsers":[]}`

- 驳回接口（http://qa.docker.workorder.sogou/task/validateOrder）：驳回后需要去未分配工单点一下（否则已处理工单无法看到接单按钮）才能看到接单按钮

- 接单接口（http://qa.docker.workorder.sogou/task/claimTask）传入orderId：

#### 问题

- 牛悦只能解释和转交，pangpeng能够撤销，驳回和关闭
- 待接单，待处理，
- 是在牛悦转交给自己之后调用接口么

#### solr

- 确定输入流：工单附件（通过启动项目，f12查看工单附件的访问地址）

  工单附件存储地址和admintask中的不一致，地址形式也不一致（admintask中的地址有.txt，.sql后缀的才会被收集），工单附件的存储地址只有定位作用，无法进行文件类型的识别

  ```
  			//查看文件流
  			ByteArrayOutputStream baos = new ByteArrayOutputStream();
  			byte[] buffer = new byte[1024];
  			int len;
  			while ((len = in.read(buffer)) > -1 ) {
  				baos.write(buffer, 0, len);
  			}
  			baos.flush();
  
  			in = new ByteArrayInputStream(baos.toByteArray());
  			InputStream inStream = new ByteArrayInputStream(baos.toByteArray());
  
  			XSSFWorkbook sheets = new XSSFWorkbook(inStream);
  			XSSFSheet sheetAt = sheets.getSheetAt(0);
  			for (int i = 0; i < sheetAt.getPhysicalNumberOfRows(); i++) {
  				XSSFRow row = sheetAt.getRow(i);
  				for (int index = 0; index < row.getPhysicalNumberOfCells(); index++) {
  					XSSFCell cell = row.getCell(index);
  					if (cell.getStringCellValue().equals("")) {
  						continue;
  					}
  					System.out.println(cell.getStringCellValue()+" ");
  				}
  				System.out.println();
  			}
  //			BufferedReader reader = new BufferedReader(new InputStreamReader(inStream,"utf-8"));
  //			StringBuilder builder = new StringBuilder();
  //
  //			String line = null;
  //			try {
  //				while ((line = reader.readLine()) != null) {
  //					builder.append(line);
  //				}
  //			} finally {
  //				try {
  //					inStream.close();
  //				} catch (IOException e) {
  //					logger.error("从dfs读取文件失败");
  //				}
  //			}
  //			System.out.println(builder.toString());
  ```

- 新建solrcore

  solrcore的创建需要conf目录

  是否能够通过远程调用solr的api创建conf目录

  - ```
            //连接solr服务器
            HttpSolrServer solrClient=new HttpSolrServer("http://10.160.45.203:9000/solr");
            System.out.println(solrClient);
            //获得默认core的路径
            String path =(String) CoreAdminRequest.getStatus("lm",solrClient).getCoreStatus().get("lm").get("instanceDir");
            //获得solrhome,也就是solr放置索引的主目录
            String solrHome=path.substring(0,path.indexOf("lm"));
            System.out.println(path);
            System.out.println(solrHome);
    ```

- schema设置，添加域

  可以通过修改文件添加设置域

  Solr中强烈推荐使用Schema API来管理集合/内核的模式信息，可以读、写模式信息。通过API来更新模式信息，solr将自动重载内核。**但是请注意：模式修改并不会自动重索引已索引的文档，只会对后续的文档起作用，如果必要，你需要手动重索引（删除原来的，重新提交文档）。**

  solr中文分词需要设定特定的分词器，查看solr服务的分词器发现已经配置了IK Analyzer中文分词器，并且建立索引和查询时都会分词，但分词的效果不一致，还在学习了解中。

  - 查看域的分词器，TokenStream是一个能够在被调用后产生语汇单元序列的类，其中有两个类型：Tokenizer和TokenFilter，两者的不同在于TokenFilter中包含了一个TokenStream作为input，该input仍然可以为一种TokenFilter进行递归封装，是一种组合模式；而Tokenzier接受一个Reader对象读取字符并创建语汇单元，TokenFilter负责处理输入的语汇单元，通过新增、删除或者修改属性的方式来产生新的语汇单元。
  - 搜索时会对域进行分词，查看搜索时的分词器

- 从数据库导入数据（权限控制）:db-data-config.xml

- 查询时的权限控制，

###### db-data-config.xml

```
数据源的配置参数：
<dataSource driver="com.mysql.jdbc.Driver" url="jdbc:mysql://xxx" user="xxx" password="xxx" />
在这个参数里还能加batchSize="-1" 这个参数的作用能防止MySQL数据过大，从而导致导入时内存溢出。建议生产环境给他配置上
```

- pk：The primary key for the entity. It is optional, and required only when using delta-imports. It has no relation to the uniqueKey defined in `schema.xml` but they can both be the same.This attribute is mandatory if you do delta-imports and then refer to the column name in `${dataimporter.delta.<column-name>}` which is used as the primary key.
- transformer：Optional. One or more transformers to be applied on this entity.
- query：Required. The SQL query used to select rows.
- deltaQuery：SQL query used if the operation is delta-import. This query selects the primary keys of the rows which will be parts of the delta-update. The pks will be available to the deltaImportQuery through the variable `${dataimporter.delta.<column-name>}`.
- deltaImportQuery：SQL query used if the operation is `delta-import`. If this is not present, DIH tries to construct the import query by (after identifying the delta) modifying the 'query' (this is error prone).There is a namespace `${dataimporter.delta.<column-name>}` which can be used in this query. For example, `select * from tbl where id=${dataimporter.delta.id}`.

##### solr启动过程

solr启动时会修改server中的solr-webapps中的内容，不知道是根据什么修改（应该是需要看启动脚本）

solr部署到tomcat：

- [solr5部署到tomcat](https://blog.csdn.net/caicongyang/article/details/50084273)
- [solr部署到tomcat](https://blog.csdn.net/qq_34630887/article/details/78517664)

##### solr目录

- bin：[Solr](javascript:;)控制台管理工具存在该目录下。如：solr.cmd等。

  ```
  ./bin/solr start [options]
  
  ./bin/solr start -help
  
  ./bin/solr restart [options]
  
  ./bin/solr restart -help
  ```

  | 参数           | 描述                                                         | 样例                                                         |
  | :------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
  | -a " <string>" | 启动Solr时，添加额外的JVM参数                                | `bin/solr start -a"-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=1044"````` |
  | -cloud         | 以SolrCloud模式启动Solr简短写法：-c                          | ./bin/solr start -c                                          |
  | -d <dir>       | 定义server目录，默认是server ( `$SOLR_HOME/server`)，一般不会重置这个目录 | ./bin/solr start -d newServerDir                             |
  | -e <name>      | 以样例配置启动Solr可选项为：cloudtechproductsdihschemaless   | `./bin/solr start -e schemaless`                             |
  | -f             | 前端方式启动Solr服务，运行样例时不可以此方式启动             | ./bin/solr start -f                                          |
  | -h <hostname>  | 启动时， 为Solr服务指定hostname, 默认为'localhost'           | ./bin/solr start -h search.mysolr.com                        |
  | -m <memory>    | 启动时， 为JVM指定最小（-Xms）和最大（-Xmx）堆大小           | ./bin/solr start -m 1g                                       |
  | -noprompt      | 启动时，接受一切默认设置，不需要交互确认                     | ./bin/solr start -e cloud -noprompt                          |
  | -p <port>      | 在指定的端口启动Solr服务，默认为'8983'                       | ./bin/solr start -p 8655                                     |
  | -s <dir>       | 设置solr.solr.home系统属性，默认server/solr.                 | ./bin/solr start -s newHome                                  |
  | -V             | 启动Solr服务时， 显示冗余信息                                | ./bin/solr start -V                                          |
  | -z <zkHost>    | 以ZooKeeper连接串启动Solr服务，本选项只在SolrCloud运行模型下使用；如果不指定本选项，则启动Solr内嵌的ZooKeeper 实例 | ./bin/solr start -c -z server1:2181,server2:2181             |

  启动时，默认设置如何，以下两个启动命令效果一样：

  ```
  ./bin/solr start
  
  ./bin/solr start -h localhost -p 8983 -d server -s solr -m 512m
  ```

- contrib：该文件包含大量关于Solr的扩展。分别如下：

  - analysis-extras：该目录下面包含一些相互依赖的文本分析组件。
  - clustering：该目录下有一个用于集群检索结果的引擎。

  - dataimporthandler：DIH是Solr中一个重要的组件，该组件可以从数据库或者其他数据源导入数据到Solr中。

  - dataimporthandler-extras：这里面包含了对DIH的扩展。

  - extraction：集成Apache Tika，用于从普通格式文件中提取文本。

  - langid：该组件使得Solr拥有在建索引之前识别和检测文档语言的能力。

  - map-reduce：提供一些和Hadoop Map-Reduce协同工作的工具。

  - morphlines-cell：为Solr Cell类型的功能提供KiteMorphlines的特性。

  - morphlines-core：为Solr提供Kite Morphlines的特性。

  - uima：该目录包含用于集成ApacheUIMA的库。

  - velocity：包含一个基于Velocity模板语言简单检索UI框架。

- dist：在这里能找到Solr的核心JAR包和扩展JAR包。当我们试图把Solr嵌入到某个应用程序的时候会用到核心JAR包。

- docs：该文件夹里面存放的是Solr文档，离线的静态HTML文件，还有API的描述。

- example：包含Solr的简单示例。在其中的exampledocs子文件夹下存放着前面提到过的用于在Windows环境下发送文档到Solr的工具post.jar程序。

- licenses：各种许可和协议。

- server：Solr Core核心必要文件都存放在这里，分别如下：        

  - contexts：启动Solr的Jetty网页的上下文配置。
  - etc：Jetty服务器配置文件，在这里可以把默认的8983端口改成其他的。
  - lib：Jetty服务器程序对应的可执行JAR包和响应的依赖包。
  - logs：默认情况下，日志将被输出到这个文件夹。
  - modules：http\https\server\ssl等配置模块。
  - resources：存放着Log4j的配置文件。这里可以改变输出日志的级别和位置等设置。
  - scripts：Solr运行的必要脚本。
  - solr：运行Solr的配置文件都保存在这里。solr.xml文件，提供全方位的配置；zoo.cfg文件，使用SolrCloud的时候有用。子文件夹/configsets存放着Solr的示例配置文件。每创建一个核心Core都会在server目录下生成相应的core名称目录。      
  - solr-webapp：Solr的平台管理界面的站点就存放在这里。
  - tmp：存放临时文件。

逻辑结构

- solr安装目录，会有运行脚本和其他扩展jar包

- solrhome：用-s指定，solrcore的所在的地方，必须有solr.xml

- solrserver：用-d指定，solr服务用jetty启动所需的文件，This directory contains an instance of the Jetty Servlet container setup to run Solr.

  If you make a copy of this example server and wish to use the 
  ExtractingRequestHandler (SolrCell), DataImportHandler (DIH), UIMA, the 
  clustering component, or any other modules in "contrib", you will need to 
  copy the required jars or update the paths to those jars in your 
  solrconfig.xml.

##### sorl查询

查询缓存

[基本查询](https://blog.csdn.net/qq_15037231/article/details/79607435)

##### coreconf

- 新建workorder_attachment目录，新建core.properties，修改corename，新建conf目录，修改datadir

- 修改schema.xml

  field标签属性

  - name：只有定义了fieldname，才能添加对应域的值
  - type：对应fieldtype标签，确定当前域是否分词，以及用什么分词器
  - indexed：是否索引
  - stored：是否存储
  - required：添加document时是否一定需要当前域
  - multiValued：当前域的值是单一值还是多个值（以数组存储）

  dynamicField：动态匹配域的name（在field标签name没有匹配的情况下）

  copyField：

##### schema.xml

分析一下analyzer的内部逻辑：
Analyzer 的内部就是一条流水线 Step 1 字符过滤（Character filter） Step 2 分词 （Tokenization） Step 3 Token 过滤（Token filtering）

ik等同于ik_max_word，会将文本做最细粒度的拆分

而 ik_smart 会做最粗粒度的拆分。

```
HTMLStripCharFilterFactory:处理html内容
```

#### 上线问题

增量同步可能一直失败，但是全量同步可能成功

```java
this.prepUrlSendHttpPost(this.params);//params被注释了，为null
```

